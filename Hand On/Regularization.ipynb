{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python390jvsc74a57bd043a9cd95aa030499ef934e4da31e07dde54f3fc3a1fd0cc4aeef7f16459ee2ac",
   "display_name": "Python 3.9.0 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Advertising.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      TV  radio  newspaper  sales\n",
       "0  230.1   37.8       69.2   22.1\n",
       "1   44.5   39.3       45.1   10.4\n",
       "2   17.2   45.9       69.3    9.3\n",
       "3  151.5   41.3       58.5   18.5\n",
       "4  180.8   10.8       58.4   12.9"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TV</th>\n      <th>radio</th>\n      <th>newspaper</th>\n      <th>sales</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>230.1</td>\n      <td>37.8</td>\n      <td>69.2</td>\n      <td>22.1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>44.5</td>\n      <td>39.3</td>\n      <td>45.1</td>\n      <td>10.4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>17.2</td>\n      <td>45.9</td>\n      <td>69.3</td>\n      <td>9.3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>151.5</td>\n      <td>41.3</td>\n      <td>58.5</td>\n      <td>18.5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>180.8</td>\n      <td>10.8</td>\n      <td>58.4</td>\n      <td>12.9</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"sales\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_converter = PolynomialFeatures(degree=3, include_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_features = polynomial_converter.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(200, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(200, 19)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "poly_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(140, 19)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 0.49300171, -0.33994238,  1.61586707,  0.28407363, -0.02568776,\n",
       "        1.49677566, -0.59023161,  0.41659155,  1.6137853 ,  0.08057172,\n",
       "       -0.05392229,  1.01524393, -0.36986163,  0.52457967,  1.48737034,\n",
       "       -0.66096022, -0.16360242,  0.54694754,  1.37075536])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poly_features[0]"
   ]
  },
  {
   "source": [
    "### *Ridge Regression*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Help on class Ridge in module sklearn.linear_model._ridge:\n\nclass Ridge(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, _BaseRidge)\n |  Ridge(alpha=1.0, *, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None)\n |  \n |  Linear least squares with l2 regularization.\n |  \n |  Minimizes the objective function::\n |  \n |  ||y - Xw||^2_2 + alpha * ||w||^2_2\n |  \n |  This model solves a regression model where the loss function is\n |  the linear least squares function and regularization is given by\n |  the l2-norm. Also known as Ridge Regression or Tikhonov regularization.\n |  This estimator has built-in support for multi-variate regression\n |  (i.e., when y is a 2d-array of shape (n_samples, n_targets)).\n |  \n |  Read more in the :ref:`User Guide <ridge_regression>`.\n |  \n |  Parameters\n |  ----------\n |  alpha : {float, ndarray of shape (n_targets,)}, default=1.0\n |      Regularization strength; must be a positive float. Regularization\n |      improves the conditioning of the problem and reduces the variance of\n |      the estimates. Larger values specify stronger regularization.\n |      Alpha corresponds to ``1 / (2C)`` in other linear models such as\n |      :class:`~sklearn.linear_model.LogisticRegression` or\n |      :class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are\n |      assumed to be specific to the targets. Hence they must correspond in\n |      number.\n |  \n |  fit_intercept : bool, default=True\n |      Whether to fit the intercept for this model. If set\n |      to false, no intercept will be used in calculations\n |      (i.e. ``X`` and ``y`` are expected to be centered).\n |  \n |  normalize : bool, default=False\n |      This parameter is ignored when ``fit_intercept`` is set to False.\n |      If True, the regressors X will be normalized before regression by\n |      subtracting the mean and dividing by the l2-norm.\n |      If you wish to standardize, please use\n |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n |      on an estimator with ``normalize=False``.\n |  \n |  copy_X : bool, default=True\n |      If True, X will be copied; else, it may be overwritten.\n |  \n |  max_iter : int, default=None\n |      Maximum number of iterations for conjugate gradient solver.\n |      For 'sparse_cg' and 'lsqr' solvers, the default value is determined\n |      by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.\n |  \n |  tol : float, default=1e-3\n |      Precision of the solution.\n |  \n |  solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'},         default='auto'\n |      Solver to use in the computational routines:\n |  \n |      - 'auto' chooses the solver automatically based on the type of data.\n |  \n |      - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n |        coefficients. More stable for singular matrices than 'cholesky'.\n |  \n |      - 'cholesky' uses the standard scipy.linalg.solve function to\n |        obtain a closed-form solution.\n |  \n |      - 'sparse_cg' uses the conjugate gradient solver as found in\n |        scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n |        more appropriate than 'cholesky' for large-scale data\n |        (possibility to set `tol` and `max_iter`).\n |  \n |      - 'lsqr' uses the dedicated regularized least-squares routine\n |        scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n |        procedure.\n |  \n |      - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n |        its improved, unbiased version named SAGA. Both methods also use an\n |        iterative procedure, and are often faster than other solvers when\n |        both n_samples and n_features are large. Note that 'sag' and\n |        'saga' fast convergence is only guaranteed on features with\n |        approximately the same scale. You can preprocess the data with a\n |        scaler from sklearn.preprocessing.\n |  \n |      All last five solvers support both dense and sparse data. However, only\n |      'sag' and 'sparse_cg' supports sparse input when `fit_intercept` is\n |      True.\n |  \n |      .. versionadded:: 0.17\n |         Stochastic Average Gradient descent solver.\n |      .. versionadded:: 0.19\n |         SAGA solver.\n |  \n |  random_state : int, RandomState instance, default=None\n |      Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\n |      See :term:`Glossary <random_state>` for details.\n |  \n |      .. versionadded:: 0.17\n |         `random_state` to support Stochastic Average Gradient.\n |  \n |  Attributes\n |  ----------\n |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n |      Weight vector(s).\n |  \n |  intercept_ : float or ndarray of shape (n_targets,)\n |      Independent term in decision function. Set to 0.0 if\n |      ``fit_intercept = False``.\n |  \n |  n_iter_ : None or ndarray of shape (n_targets,)\n |      Actual number of iterations for each target. Available only for\n |      sag and lsqr solvers. Other solvers will return None.\n |  \n |      .. versionadded:: 0.17\n |  \n |  See Also\n |  --------\n |  RidgeClassifier : Ridge classifier.\n |  RidgeCV : Ridge regression with built-in cross validation.\n |  :class:`~sklearn.kernel_ridge.KernelRidge` : Kernel ridge regression\n |      combines ridge regression with the kernel trick.\n |  \n |  Examples\n |  --------\n |  >>> from sklearn.linear_model import Ridge\n |  >>> import numpy as np\n |  >>> n_samples, n_features = 10, 5\n |  >>> rng = np.random.RandomState(0)\n |  >>> y = rng.randn(n_samples)\n |  >>> X = rng.randn(n_samples, n_features)\n |  >>> clf = Ridge(alpha=1.0)\n |  >>> clf.fit(X, y)\n |  Ridge()\n |  \n |  Method resolution order:\n |      Ridge\n |      sklearn.base.MultiOutputMixin\n |      sklearn.base.RegressorMixin\n |      _BaseRidge\n |      sklearn.linear_model._base.LinearModel\n |      sklearn.base.BaseEstimator\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, alpha=1.0, *, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  fit(self, X, y, sample_weight=None)\n |      Fit Ridge regression model.\n |      \n |      Parameters\n |      ----------\n |      X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n |          Training data\n |      \n |      y : ndarray of shape (n_samples,) or (n_samples, n_targets)\n |          Target values\n |      \n |      sample_weight : float or ndarray of shape (n_samples,), default=None\n |          Individual weights for each sample. If given a float, every sample\n |          will have the same weight.\n |      \n |      Returns\n |      -------\n |      self : returns an instance of self.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __abstractmethods__ = frozenset()\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.base.RegressorMixin:\n |  \n |  score(self, X, y, sample_weight=None)\n |      Return the coefficient of determination :math:`R^2` of the\n |      prediction.\n |      \n |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n |      can be negative (because the model can be arbitrarily worse). A\n |      constant model that always predicts the expected value of `y`,\n |      disregarding the input features, would get a :math:`R^2` score of\n |      0.0.\n |      \n |      Parameters\n |      ----------\n |      X : array-like of shape (n_samples, n_features)\n |          Test samples. For some estimators this may be a precomputed\n |          kernel matrix or a list of generic objects instead with shape\n |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n |          is the number of samples used in the fitting for the estimator.\n |      \n |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n |          True values for `X`.\n |      \n |      sample_weight : array-like of shape (n_samples,), default=None\n |          Sample weights.\n |      \n |      Returns\n |      -------\n |      score : float\n |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n |      \n |      Notes\n |      -----\n |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n |      with default value of :func:`~sklearn.metrics.r2_score`.\n |      This influences the ``score`` method of all the multioutput\n |      regressors (except for\n |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.linear_model._base.LinearModel:\n |  \n |  predict(self, X)\n |      Predict using the linear model.\n |      \n |      Parameters\n |      ----------\n |      X : array-like or sparse matrix, shape (n_samples, n_features)\n |          Samples.\n |      \n |      Returns\n |      -------\n |      C : array, shape (n_samples,)\n |          Returns predicted values.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.base.BaseEstimator:\n |  \n |  __getstate__(self)\n |  \n |  __repr__(self, N_CHAR_MAX=700)\n |      Return repr(self).\n |  \n |  __setstate__(self, state)\n |  \n |  get_params(self, deep=True)\n |      Get parameters for this estimator.\n |      \n |      Parameters\n |      ----------\n |      deep : bool, default=True\n |          If True, will return the parameters for this estimator and\n |          contained subobjects that are estimators.\n |      \n |      Returns\n |      -------\n |      params : dict\n |          Parameter names mapped to their values.\n |  \n |  set_params(self, **params)\n |      Set the parameters of this estimator.\n |      \n |      The method works on simple estimators as well as on nested objects\n |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n |      parameters of the form ``<component>__<parameter>`` so that it's\n |      possible to update each component of a nested object.\n |      \n |      Parameters\n |      ----------\n |      **params : dict\n |          Estimator parameters.\n |      \n |      Returns\n |      -------\n |      self : estimator instance\n |          Estimator instance.\n\n"
     ]
    }
   ],
   "source": [
    "help(Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model = Ridge(alpha=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Ridge(alpha=10)"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "ridge_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = ridge_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE = mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5774404204714167"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = np.sqrt(mean_squared_error(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.894638646131965"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv_model = RidgeCV(alphas=(0.1, 1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv_model = RidgeCV(alphas=(0.1, 1, 10), scoring='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RidgeCV(alphas=array([ 0.1,  1. , 10. ]), scoring='neg_mean_absolute_error')"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "ridge_cv_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "ridge_cv_model.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import SCORERS # for examine SCORERES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['explained_variance', 'r2', 'max_error', 'neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_absolute_percentage_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_root_mean_squared_error', 'neg_mean_poisson_deviance', 'neg_mean_gamma_deviance', 'accuracy', 'top_k_accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo', 'roc_auc_ovr_weighted', 'roc_auc_ovo_weighted', 'balanced_accuracy', 'average_precision', 'neg_log_loss', 'neg_brier_score', 'adjusted_rand_score', 'rand_score', 'homogeneity_score', 'completeness_score', 'v_measure_score', 'mutual_info_score', 'adjusted_mutual_info_score', 'normalized_mutual_info_score', 'fowlkes_mallows_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted'])"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "SCORERS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = ridge_cv_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE = mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = np.sqrt(mean_squared_error(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4273774884351013"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6180719926948981"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 5.40769392,  0.5885865 ,  0.40390395, -6.18263924,  4.59607939,\n",
       "       -1.18789654, -1.15200458,  0.57837796, -0.1261586 ,  2.5569777 ,\n",
       "       -1.38900471,  0.86059434,  0.72219553, -0.26129256,  0.17870787,\n",
       "        0.44353612, -0.21362436, -0.04622473, -0.06441449])"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "ridge_cv_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-0.3749223340292953"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "ridge_cv_model.best_score_"
   ]
  },
  {
   "source": [
    "### *Lasso Regression*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_cv_model = LassoCV(eps=0.001, n_alphas=100, cv=5, max_iter=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LassoCV(cv=5, max_iter=1000000)"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "lasso_cv_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_cv_model = LassoCV(eps=0.1, n_alphas=100, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LassoCV(cv=5, eps=0.1)"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "lasso_cv_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4943070909225832"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "lasso_cv_model.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = lasso_cv_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE = mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = np.sqrt(mean_squared_error(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6541723161252867"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.1308001022762548"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1.002651  , 0.        , 0.        , 0.        , 3.79745279,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "lasso_cv_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], eps=0.001, n_alphas=100, max_iter=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ElasticNetCV(l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1], max_iter=1000000)"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "elastic_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1]"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "elastic_model.l1_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "elastic_model.l1_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.004943070909225833"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "elastic_model.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4943070909225832"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "lasso_cv_model.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = elastic_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE = mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = np.sqrt(mean_squared_error(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4335034618590078"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6063140748984043"
      ]
     },
     "metadata": {},
     "execution_count": 84
    }
   ],
   "source": [
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}