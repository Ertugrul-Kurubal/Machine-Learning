{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"CLRSWY\"></p>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WELCOME!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to \"***Tree Types Prediction Project***\". This is the second medium project of ***Machine Learning*** course.\n",
    "\n",
    "In this project, you must apply ***EDA*** processes for the development of predictive models. Handling outliers, domain knowledge and feature engineering (using ***sqlite3*** library) will be challenges. \n",
    "\n",
    "Also, this project aims to improve your ability to implement algorithms for ***Multi-Class Classification***. Thus, you will have the opportunity to implement many algorithms commonly used for Multi-Class Classification problems.\n",
    "\n",
    "Before diving into the project, please take a look at the determines and tasks.\n",
    "\n",
    "- ***NOTE:*** *This project assumes that you already know the basics of coding in Python. You should also be familiar with the theory behind classification models and scikit-learn module as well as Machine Learning before you begin.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Determines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset contains tree observations from four areas of one national forest district. This dataset includes information on tree type, shadow coverage, distance to nearby landmarks, soil type, and local topography. The goal of the project is to build a model that predicts what types of trees grow in an area.\n",
    "***The Forest Dataset*** contains approximately 600 thousand lines, also you can easily find many information about it on the web (especially Kaggle).\n",
    "\n",
    "---\n",
    "\n",
    "To achieve high prediction success, you must understand the data well and develop different approaches that can affect the dependent variable.\n",
    "\n",
    "Firstly, try to understand the dataset column by column using pandas module. Do research within the scope of domain (forest, trees) knowledge on the internet to get to know the data set in the fastest way. \n",
    "\n",
    "You should implement cleaning, handling with outliers and missing values using Pandas, NumPy and other required modules for the best result in modeling. You should do Feature Engineering using SQLite local database. \n",
    "\n",
    "At this point, in order to improve your skills of using SQL with Python, you are asked to perform feature engineering operations using *sqlite3* library in Python.\n",
    "\n",
    "After that, your final dataset with the new variables you have created will be ready for model building. You will implement ***Support Vector Machine, XGBoost, Random Forest, Desicion Tree*** and ***K-Nearest Neighbors (KNN)*** algorithms. Also, evaluate the success of your models with appropriate performance metrics and visualize them using ***Yellowbrick, Seaborn*** or ***Matplotlib*** modules.\n",
    "\n",
    "At the end of the project, create a chart comparing the performance of all models and choose the most successful model.\n",
    "\n",
    "- ***NOTE:*** *Evaluate your models knowing that this is [imbalanced data](https://towardsdatascience.com/having-an-imbalanced-dataset-here-is-how-you-can-solve-it-1640568947eb). This is not the primary goal of the project, but you can study solve the class [imbalance problem](https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28) if you want.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Exploratory Data Analysis (EDA)\n",
    "- Import Libraries, Load Dataset, Exploring Data\n",
    "\n",
    "    *i. Import Libraries*\n",
    "    \n",
    "    *ii. Load Dataset*\n",
    "    \n",
    "    *iii. Explore Data*\n",
    "\n",
    "#### 2.  Data Cleaning\n",
    "- Detect Missing Values and Outliers \n",
    "\n",
    "    *i. Missing Value Detection*\n",
    "    \n",
    "    *ii. Outlier Detection*\n",
    "    \n",
    "- Deal with Outliers\n",
    "    \n",
    "    *i. Visualize Zscore Tresholds (how many times IQR) by Continuous Variables*\n",
    "    \n",
    "    *ii. Drop Outliers*\n",
    "\n",
    "\n",
    "#### 3. Feature Engineering (if needed)\n",
    "\n",
    "\n",
    "#### 4. Prediction (Multi-class Classification)\n",
    "- Import libraries\n",
    "- Data Preprocessing\n",
    "- Implement XGBoost Classifer\n",
    "- Implement SVM Classifer\n",
    "- Implement Decision Tree Classifier\n",
    "- Implement KNN Classifer\n",
    "- Implement Random Forest Classifer\n",
    "- Compare The Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AZiiwrgri2o"
   },
   "source": [
    "### Import Libraries, Load Dataset, Exploring Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *i. Import Libraries*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides Numpy and Pandas, you need to import the necessary modules for data visualization, data preprocessing, Model building and tuning.\n",
    "\n",
    "*Note: Check out the course materials.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *ii. Load Dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *iii. Explore Data*\n",
    "- Focus on numerical and categorical data\n",
    "- Detect Number of Unique values of each column\n",
    "- Focus on Target Variable (Cover_Type)\n",
    " - Detect relationships and correlations between independent variables and target variable.\n",
    " - It may be nice to visualize the class frequencies of the target variable.\n",
    "- Detect relationships and correlations between independent variables. (You can prefer to keep only one of the highly correlated continuous variables.)\n",
    "- Consider dropping features that contain little data or that you think will not contribute to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQeP_Dhtri3v"
   },
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Missing Values and Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *i. Missing Value Detection*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *ii. Outlier Detection*\n",
    "\n",
    "The columns which have continuous value should be examined in terms of [outliers](https://datascience.foundation/sciencewhitepaper/knowing-all-about-outliers-in-machine-learning) (Watch out for columns that look like continuous but not continuous!). Some algorithms are [sensitive to outliers](https://arsrinevetha.medium.com/ml-algorithms-sensitivity-towards-outliers-f3862a13c94d), but some algorithms can tolerate them. You can decide to outlier detection according to the algorithm you will use.\n",
    "- You can check the outliers shape of continous features with respect to the target (Cover_Type) classes.\n",
    "- You can check how many outliers are there of each continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ax0-S9b6ri3I"
   },
   "source": [
    "### Deal with Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQPOenOmri3O"
   },
   "source": [
    "#### *i. Visualize Zscore Tresholds (how many times IQR) by Continuous Variables*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different methods for outliers. You can use IQR values used as standard to deal with outliers, or you can define two functions to help you understand the outliers and how you can deal with them.\n",
    "- Two functions given as extra for outlier detection are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 5204,
     "status": "ok",
     "timestamp": 1605345898768,
     "user": {
      "displayName": "Martin Lane",
      "photoUrl": "",
      "userId": "14451751373132005694"
     },
     "user_tz": -60
    },
    "id": "28KUmwCLri3L"
   },
   "outputs": [],
   "source": [
    "def outlier_zscore(df, col, min_z=1, max_z = 5, step = 0.1, print_list = False):\n",
    "    z_scores = stats.zscore(df[col].dropna())\n",
    "    threshold_list = []\n",
    "    for threshold in np.arange(min_z, max_z, step):\n",
    "        threshold_list.append((threshold, len(np.where(z_scores > threshold)[0])))\n",
    "        df_outlier = pd.DataFrame(threshold_list, columns = ['threshold', 'outlier_count'])\n",
    "        df_outlier['pct'] = (df_outlier.outlier_count - df_outlier.outlier_count.shift(-1))/df_outlier.outlier_count*100\n",
    "    plt.plot(df_outlier.threshold, df_outlier.outlier_count)\n",
    "    best_treshold = round(df_outlier.iloc[df_outlier.pct.argmax(), 0],2)\n",
    "    outlier_limit = int(df[col].dropna().mean() + (df[col].dropna().std()) * df_outlier.iloc[df_outlier.pct.argmax(), 0])\n",
    "    percentile_threshold = stats.percentileofscore(df[col].dropna(), outlier_limit)\n",
    "    plt.vlines(best_treshold, 0, df_outlier.outlier_count.max(), \n",
    "               colors=\"r\", ls = \":\"\n",
    "              )\n",
    "    plt.annotate(\"Zscore : {}\\nValue : {}\\nPercentile : {}\".format(best_treshold, outlier_limit, \n",
    "                                                                   (np.round(percentile_threshold, 3), \n",
    "                                                                    np.round(100-percentile_threshold, 3))), \n",
    "                 (best_treshold, df_outlier.outlier_count.max()/2))\n",
    "    #plt.show()\n",
    "    if print_list:\n",
    "        print(df_outlier)\n",
    "    return (plt, df_outlier, best_treshold, outlier_limit, percentile_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 5203,
     "status": "ok",
     "timestamp": 1605345898769,
     "user": {
      "displayName": "Martin Lane",
      "photoUrl": "",
      "userId": "14451751373132005694"
     },
     "user_tz": -60
    },
    "id": "Uu2x64KDri3N"
   },
   "outputs": [],
   "source": [
    "def outlier_inspect(df, col, min_z=1, max_z = 5, step = 0.5, max_hist = None, bins = 50):\n",
    "    fig = plt.figure(figsize=(20, 6))\n",
    "    fig.suptitle(col, fontsize=16)\n",
    "    plt.subplot(1,3,1)\n",
    "    if max_hist == None:\n",
    "        sns.distplot(df[col], kde=False, bins = 50)\n",
    "    else :\n",
    "        sns.distplot(df[df[col]<=max_hist][col], kde=False, bins = 50)\n",
    "    plt.subplot(1,3,2)\n",
    "    sns.boxplot(df[col])\n",
    "    plt.subplot(1,3,3)\n",
    "    z_score_inspect = outlier_zscore(df, col, min_z=min_z, max_z = max_z, step = step)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *ii. Drop Outliers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can define another function to detect outliers in accordance with the ``zscore`` (how many times IQR) value you choose according to the result from the previous functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQeP_Dhtri3v"
   },
   "source": [
    "***\n",
    "***"
   ]
  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Feature Engineering(If needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ylwp86BnIbfD"
   },
   "source": [
    "## 4. Prediction (Multi-class Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have done, use your data set resulting from Feature Engineering task. If you haven't done Feature Engineering, use the latest version of your data set.\n",
    "In this section, you have two main tasks that apply to each algorithm:\n",
    "1. Model Building and Prediction\n",
    "\n",
    " - XGBoost (Use ``XGBClassifier`` model from``xgboost`` module)\n",
    " - SVM (Use ``LinearSVC`` model from``sklearn.svm`` module)\n",
    " - Decision Tree (Use ``DecisionTreeClassifier`` model from ``sklearn.tree`` module)\n",
    " - KNN (Use ``KNeighborsClassifier`` model from ``sklearn.neighbors`` module)\n",
    " - Random Forest (Use ``RandomForestClassifier`` model from ``sklearn.ensemble`` module) \n",
    " \n",
    "\n",
    "2. Visualizing the Result\n",
    "\n",
    "- Use [yellowbrick](https://www.scikit-yb.org/en/latest/), [seaborn](https://seaborn.pydata.org/tutorial/regression.html) or [matplotlib](https://matplotlib.org/) modules to visualize the model results.\n",
    "\n",
    "- Show three plots for the results:\n",
    " - Class Prediction Error Bar Plot\n",
    " - Confusion Matrix\n",
    " - Classification Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-HeuK9FIbfJ"
   },
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Drop target variable\n",
    "- Train-Test Split\n",
    "\n",
    "*Note: You can use the train and test data generated here for all algorithms.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eI7Zd8TLIbfK"
   },
   "source": [
    "### Implement XGBoost Classifer\n",
    "- Import the modul \n",
    "- Fit the model \n",
    "- Predict the test set\n",
    "- Visualize and evaluate the result (use yellowbrick module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eI7Zd8TLIbfK"
   },
   "source": [
    "### Implement Support Vector Machine\n",
    "- Import the modul \n",
    "- Fit the model \n",
    "- Predict the test set\n",
    "- Visualize and evaluate the result (use yellowbrick module)\n",
    "\n",
    "*Note: You probably won't get a successful result. You may need to make some changes to the model or data. This may be a topic worth investigating, you decide.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eI7Zd8TLIbfK"
   },
   "source": [
    "### Implement Decision Tree Classifier\n",
    "- Import the modul \n",
    "- Fit the model \n",
    "- Predict the test set\n",
    "- Visualize and evaluate the result (use yellowbrick module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eI7Zd8TLIbfK"
   },
   "source": [
    "### Implement KNeighborsClassifer\n",
    "\n",
    "The first and most important step for the [KNN](https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/) algorithm is to determine the optimal k (number of neighbors). \n",
    "\n",
    "Build different models with k values in the range you specify. You can observe the change of train and test accuracy values according to different k values using a plot. The point at which train accuracy and test accuracy values begin to run parallel is the optimal k value. Then set up your final KNN model with the optimal k value you determined and calculate accuracy.\n",
    "\n",
    "- Import the modul\n",
    "- Fit the model \n",
    "- Predict the test set\n",
    "- Visualize the result\n",
    "- Evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eI7Zd8TLIbfK"
   },
   "source": [
    "### Implement Random Forest Classifier\n",
    "- Import the modul \n",
    "- Fit the model \n",
    "- Predict the test set\n",
    "- Visualize and evaluate the result (use yellowbrick module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare The Models\n",
    "\n",
    "So far, you have created a multi-classifier model with 5 different algorithms and made predictions. You can observe the performance of the models together with a barplot of your choice.\n",
    "\n",
    "- Which algorithm did you achieve the highest prediction performance with? \n",
    "- What could be the factors that cause this? What are the principles of the most successful algorithm and its differences from other algorithms? \n",
    "\n",
    "In contrast;\n",
    "\n",
    "- Which algorithm did you achieve the lowest prediction performance with? \n",
    "- What could be the factors that cause this? What are the principles of the most successful algorithm and its differences from other algorithms? \n",
    "\n",
    "The answers you will look for to these questions will increase your gains from Machine Learning course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"CLRSWY\"></p>\n",
    "\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "01_ForestProject-EDA-1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
